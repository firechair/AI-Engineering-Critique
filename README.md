# AI Engineering Critique

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Streamlit App](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://streamlit.io)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)

> "I don't just use AIâ€”I systematically evaluate, improve, and architect AI solutions."

## ğŸ¯ Project Overview

This repository showcases a systematic approach to AI Engineering, focusing on rigorous evaluation, prompt engineering, and architectural thinking. It demonstrates how to move beyond simple prompting to build reliable, high-quality AI systems through iterative improvement and structured critique.

The project highlights expertise in:
- **AI Output Evaluation**: Systematically assessing AI responses using multi-dimensional rubrics.
- **AI-Generated Code Review**: Applying architectural principles to review and improve AI code.
- **Prompt Design & Analysis**: Analyzing prompts to identify weaknesses and applying techniques for better results.
- **Systematic Thinking**: Using frameworks and checklists to ensure consistent quality.

## ğŸ“‹ Project Goals

This repository aims to demonstrate:
1. âœ… **Reasoning**: How I reason about AI outputs and identify subtleties.
2. âœ… **Evaluation**: How I evaluate systems systematically using defined criteria.
3. âœ… **Iteration**: How I improve AI outputs through iterative refinement.
4. âœ… **Architecture**: How I think like a reviewer/architect when designing AI solutions.

## ğŸ—ï¸ Repository Structure

```
ai-engineering-critique/
â”œâ”€â”€ streamlit-app/                     # ğŸš€ Interactive evaluation tool
â”‚   â”œâ”€â”€ app.py                         # Main application
â”‚   â”œâ”€â”€ rubrics/                       # YAML definitions for evaluation criteria
â”‚   â”œâ”€â”€ prompt_techniques/             # Techniques for prompt enhancement
â”‚   â””â”€â”€ utils/                         # Helper logic for LLM, analysis, etc.
â”‚
â”œâ”€â”€ case-studies/                      # ğŸ“š Deep-dive analyses of specific problems
â”‚   â””â”€â”€ 001-rate-limiter-design/       # Example: Designing a rate limiter
â”‚
â”œâ”€â”€ prompt-iterations/                 # ğŸ”„ Journeys of prompt refinement
â”‚   â””â”€â”€ 001-api-documentation/         # Example: Improving API docs
â”‚
â”œâ”€â”€ framework/                         # ğŸ§  Methodology and guides
â”‚   â”œâ”€â”€ evaluation-criteria.md         # Standard dimensions for evaluation
â”‚   â”œâ”€â”€ prompt-patterns.md             # Effective prompt patterns
â”‚   â””â”€â”€ review-checklist.md            # Systematic review process
â”‚
â””â”€â”€ docs/                              # ğŸ“– Usage documentation
    â””â”€â”€ how-to-use-app.md              # Guide for the Streamlit app
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.10 or higher
- An Anthropic API key (for the AI features)

### Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/yourusername/ai-engineering-critique.git
   cd ai-engineering-critique
   ```

2. Install dependencies:
   ```bash
   pip install -r streamlit-app/requirements.txt
   ```

3. Set up environment variables:
   Create a `.env` file in the root or export the key directly:
   ```bash
   export ANTHROPIC_API_KEY="your-api-key"
   ```

### Running the App

Launch the Streamlit application to start evaluating and improving prompts:

```bash
streamlit run streamlit-app/app.py
```

## ğŸ¨ Features

The interactive Streamlit application included in this repository allows you to:

- **Compare Responses**: Generate and evaluate two AI responses side-by-side.
- **Build Rubrics**: Create custom evaluation criteria or use pre-defined templates.
- **Enhance Prompts**: Get automated suggestions to improve your prompts using proven techniques.
- **Track Scores**: Quantify performance across dimensions like Correctness, Clarity, and Best Practices.

## ğŸ“š Documentation

- [**Framework**](./framework/README.md): Detailed explanation of the evaluation methodology.
- [**Case Studies**](./case-studies/README.md): Real-world examples of this methodology in action.
- [**Prompt Iterations**](./prompt-iterations/README.md): Logs of how prompts evolved to solve specific problems.

## ğŸ¤ Contributing

Contributions are welcome! Please check [CONTRIBUTING.md](./docs/contributing.md) for guidelines.

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
